# 1. Load the train data K folds and the performance metric function for validation

Let's read the *Data.RData* file created in *3.1. Supervised ML Validation Protocol*.
```{r}
load("Data.RData")
```

Also, assign every feature to its type: *ID*, *nominal* or *categorical*, *numeric*, and *target* or variable to predict.
```{r}
features_id <- c("ID")
features_nominal <- c("Civil_Status","Children","Profession","Buy_A","Buy_B","Buy_C","Buy_D","Buy_E",
                      "Number_Products","Interest","Card","Gender","Habitat_Type")
features_numeric <- c("Age","Seniority","Time_Without_Buying_B","Total_Expenses","Time_Without_Buying")
target <- "Class"
```

# 2. K-Fold Cross-validation for all parametrizations

Let's iterate the train data K folds, training each parametrization of *KNN* in the other folds and evaluating the performance metric *AUC* in the actual fold. It will give as 5 estimations of *AUC* in *test* data because we are using out of bag observations (OOB) that have not been used for training the model.

*Note:* You can find more information about this validation process in the section *4.5. Phases of the protocol of model validation* of the file *Guide to Spark Machine Learning for credit scoring.pdf* in my GitHub repository *Guide-to-Spark-Machine-Learning-for-credit-scoring*.
```{r}
# Load the library of the model
library(VIM)

# Intial time
start.time <- Sys.time()

# List of candidate parametrizations
neigbours <- seq(1,10,2)
parametrizations <- expand.grid(c("Nominal_K","Numeric_K","All_K"),neigbours)

# Iterate all train data folds
K <- 5
results_cv_KNN <- data.frame(row.names=paste0(parametrizations[,1],parametrizations[,2]))
for(i in 1:K){
  
  # Create the train and test sets for the ith fold
  train_i <- rbind(data_train_folds[[c(1:K)[-i][1]]],data_train_folds[[c(1:K)[-i][2]]],
                   data_train_folds[[c(1:K)[-i][3]]],data_train_folds[[c(1:K)[-i][4]]])
  test_i <- data_train_folds[[i]]
  
  # Impute NA in test_i target variable for the model to impute predictions
  test_target_copy <- test_i[,target]
  test_i[,target] <- NA
  data_i <- rbind(train_i,test_i)
  
  # Iterate all candidate parametrizations
  for(k in neigbours){
  
    # Model with numeric features and k neighbours
    model_KNN_1_i <- kNN(data_i,variable=target,numFun=mean,k=k,dist_var=features_numeric)
    prediction_KNN_1_i <- model_KNN_1_i[model_KNN_1_i[,paste0(target,"_imp")]==TRUE,target]
    results_cv_KNN[paste0("Numeric_K",k),i] <- AUC(test_target_copy,prediction_KNN_1_i)
    
    # Model with nominal features and k neighbours
    model_KNN_2_i <- kNN(data_i,variable=target,numFun=mean,k=k,dist_var=features_nominal)
    prediction_KNN_2_i <- model_KNN_2_i[model_KNN_2_i[,paste0(target,"_imp")]==TRUE,target]
    results_cv_KNN[paste0("Nominal_K",k),i] <- AUC(test_target_copy,prediction_KNN_2_i)
    
    # Model with all features and k neighbours
    model_KNN_3_i <- kNN(data_i,variable=target,numFun=mean,k=k,dist_var=c(features_numeric,features_nominal))
    prediction_KNN_3_i <- model_KNN_3_i[model_KNN_3_i[,paste0(target,"_imp")]==TRUE,target]
    results_cv_KNN[paste0("All_K",k),i] <- AUC(test_target_copy,prediction_KNN_3_i)
  }  
}
duration <- Sys.time()-start.time

# Mean the K AUC for each parametrization
results_cv_KNN[,K+1] <- apply(results_cv_KNN,1,mean)
names(results_cv_KNN) <- c(paste0("Fold_",1:K),"Mean_AUC")

# Order the results by highest mean AUC
results_cv_KNN <- results_cv_KNN[order(-results_cv_KNN$Mean_AUC),]
```

Let's see the duration of the validation protocol.
```{r}
duration
```

# 3. Save results into RData and find the best parametrization

The best parametrization is: all features and K parameter equal to 1.
```{r}
save(results_cv_KNN,file="Outputs/3.2. Supervised ML KNN Results Validation.RData")
results_cv_KNN[1,]
```